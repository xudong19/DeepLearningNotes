[Geoffrey Hinton and Yann LeCun to Deliver Turing Lecture](https://www.youtube.com/watch?v=VsnQf7exv5I)
------

###### __Discription__:
This are the lectures given by Geoffrey Hinton and Yann LeCUn in ACM FCRC as Turing lecture in 2019.

<br/> 
###### __Note__:

Geoffrey:
*Using gradient is the fastest way of learning. Symbolic reasoning didnâ€™t work because gradient is not used.
*Now we are more ready to solve the reasoning problem. We were not ready in 1980s.
*About vision: Current CNN relies too much on texture and color, but not geometrical relationship, which human beings relies on a lot!
*What is we can impose coordinate frame into learning process? It can be more data-efficient. https://arxiv.org/abs/1906.06818
*How to restore memory is very relevant to reasoning. Memory has to be stored in the neuron nets in a recursive way. This might become fashionable in the future.


Yann:

Theory is behind the practice. A lot of similar things happened in science history.
Self-supervised learning is the main bulk of the intelligence
Self-supervised learning is about prediction (filling the blanks).
It is easier to do for language models but hard to do for vision. It is because for vision it is continuous space and there are uncertainty in the prediction.
One way to achieve self-supervised learning in vision is by imposing random variables. 
There is a concrete example of making car to learn how to drive by looking at high-way videos. There are tricks behind it. (https://arxiv.org/abs/1901.02705)
For fully autonomous driving vehicles, it is not practical to only use current DL technique and simply scale the training data to achieve it. The performance discrepancy between current SOTA and human performance is too big.
There is gotta to be new ways for learning to achieve self-driving cars! 
