[Dreamitate: Real-World Visuomotor Policy Learning via Video Generation](https://arxiv.org/pdf/2406.16862) [CoRL 24]
---------------	

__TL;DR__: Take full advantage of the video generation ability of diffusion models, and attract actions from videos and transfer them into robots

__keywords__: bla-bla

__Resources__: [[Github](https://github.com/cvlab-columbia/dreamitate)] 

__Other Notable Info__: [blabla](blabla)

<br/>    

General Comments:
------
* Let diffusion models to generate a video about how a human would do a task using specified tools
* Then extract the actions of the tools and directly apply them into robots
* it feels to me the whole rationale behind the approach is that SOTA models are too good at video generation and thereforce we can take full advantage of it. But that's not how human do planning tasks

Key ideas and technical details:
------
* Stereo cameras for vidoes generation, which makes it easier to extract actions.
* conditioned on specified tools and known CAT models 

Other noteworthy points:
------
* 
* 

Screenshots:
------
<!-- ![Image1](../img/pointnet_net.png "Architecture") -->

