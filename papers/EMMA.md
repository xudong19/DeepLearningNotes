[EMMA: End-to-End Multimodal Model for Autonomous Driving](https://arxiv.org/pdf/2410.23262) 
---------------	

__TL;DR__: blablablablabla

__keywords__: bla-bla

__Resources__: [[Github](blabla)] 

__Other Notable Info__: [blabla](blabla)

<br/>    

General Comments:
------
* Use pretrained Gemini, keep input and output as text representation, no changes.
* for input, add ego history and router info. router info is represented as text
like turnning right.
* for output, use waypoint as trajectory, adding chain of thought descriptions as output.
* the COT is automatically generated by internal off-the-shelve perception models and original
Gemini model.
* scene description, critical objects, behavior description of critical objects, and meta driving decision, which is very much like the perception, prediction, and decision making in the traditional
AV stack.
* on top of CoT, there are also 3D object detection, road graph estimation, and scene
understanding tasks to predict, with different prompting as inputs.


Key ideas and technical details:
------
* I feel the key is to predict more stuff, just like multi-task learning.
* 

Other noteworthy points:
------
* 
* 

Screenshots:
------
<!-- ![Image1](../img/pointnet_net.png "Architecture") -->

