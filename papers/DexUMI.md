[DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation](https://arxiv.org/pdf/2505.21864)
---------------	

__TL;DR__: blablablablabla

__keywords__: bla-bla

__Resources__: [[Github](https://github.com/real-stanford/DexUMI)] 

__Other Notable Info__: [Project Page](https://dex-umi.github.io/)

<br/>    

General Comments:
------
* Exoskeleton for human to do human data collection
* Exoskeleton is tailered/optimized to mimic the target robot hand, and at the same
time to be wearable and easy to operate by human
* mount vision system on the wrist
* Collect human data wearing exoskeleton to:
   - record human(exoskeleton) proprioceptive data and retarget to robot hand
   - build a vision pipeline to replace human hand with robot hand in the vision data
   - using phone camera for wrist tracking
   - additional tactile sensing applied onto exoskeleton and robot hand to ensure consistency
* once the data is transferred into robot hand data, then do straightforward imitation learning

Key ideas and technical details:
------
* 
* 

Other noteworthy points:
------
* relative action space is consistently better
* tactile sensing has no good use so far.

Screenshots:
------
<!-- ![Image1](../img/pointnet_net.png "Architecture") -->

